# -*- coding: utf-8 -*-
"""TL model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wjHHgp-LENgdYXeeak0M86DB37mPjHci

Mount to Google Drive
"""

# Mount Google Drive to access your data
from google.colab import drive
drive.mount('/content/drive')

"""Model Building using DenseNet CNN And Model Evaluting"""

# Import necessary libraries
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import GlobalAveragePooling2D,Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Define paths to your data folders
train_data_dir = "/content/drive/MyDrive/PROJECT/DATASET/Resized_Images/Training"
test_data_dir = "/content/drive/MyDrive/PROJECT/DATASET/Resized_Images/Testing"
num_classes = len(os.listdir(train_data_dir))


# Define the batch size and image size
batch_size = 32
image_size = (280,280)  # Adjust this to match the input size expected by your chosen model

# Create data generators for training and testing data
train_datagen = ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    rescale=1./255,  # Normalize pixel values to [0, 1]
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical'  # Change to 'binary' if it's a binary classification task
)

test_generator = test_datagen.flow_from_directory(
    test_data_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical'  # Change to 'binary' if it's a binary classification task
)

# Load a pre-trained CNN model (e.g., ResNet, Inception, VGG, etc.)
base_model = DenseNet121  (weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))


# Unfreeze specific layers (for example, layers from 10 to 15)
for layer in base_model.layers[6:]:
    layer.trainable = True

# Add custom layers for your specific classification task
#conv layers are used for feature extraction
#pooling layer is used to reduce the dimension
x = base_model.output
x=Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x=MaxPooling2D(2, 2)(x)
x=Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x=MaxPooling2D(2, 2)(x)
x=Conv2D(256, (3, 3), activation='relu', padding='same')(x)
x = GlobalAveragePooling2D()(x)
#To convert the output of above layers to 1D
x=Flatten()(x)
x= Dropout(0.5)(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(num_classes, activation='softmax')(x)  # num_classes is the number of classes in your dataset

# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
epochs = 15  # Adjust as needed
model.fit(
    train_generator,
    epochs=epochs,
    steps_per_epoch=len(train_generator),
    validation_data=test_generator,
    validation_steps=len(test_generator)
)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Save the trained model if needed
# model.save('/content/drive/My Drive/trained_model.h5')

"""Testing Using Other Data"""

from PIL import Image
import numpy as np

# Load the image (replace 'new_image.jpg' with your image file path)
img_path="/content/drive/MyDrive/PROJECT/DATASET/OtherTestData/test17_m.jpg"
img = Image.open(img_path)

# Preprocess the image (e.g., resize it to match the model's input size)
img = img.resize((280, 280))  # Adjust the size as needed

# Convert the image to a NumPy array and preprocess it further (e.g., normalize)
img_array = np.array(img)
img_array = img_array / 255.0  # Normalize pixel values (if required)

# Expand dimensions to match the model's input shape (e.g., if the model expects batches)
img_array = np.expand_dims(img_array, axis=0)

# Perform the prediction using the model
predictions = model.predict(img_array)
print("Predictions:",predictions)

# Interpret the predictions (e.g., for a classification task)
predicted_class = np.argmax(predictions, axis=1)
print("Predicted Class:",predicted_class)

"""Saving The Model"""

model.save('/content/drive/My Drive/PROJECT/MODEL.h5')

